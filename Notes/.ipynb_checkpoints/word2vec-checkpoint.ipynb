{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d27de6f2-ceb0-4242-be8d-d18f6624d4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ['The future king is the prince',\n",
    "'Daughter is the princess',\n",
    "'Son is the prince',\n",
    "'Only a man can be a king',\n",
    "'Only a woman can be a queen',\n",
    "'The princess will be a queen',\n",
    "'Queen and king rule the realm',\n",
    "'The prince is a strong man',\n",
    "'The princess is a beautiful woman',\n",
    "'The royal family is the king and queen and their children',\n",
    "'Prince is only a boy now',\n",
    "'A boy will be a man']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b830358-2914-43a1-a883-4bd82561751a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(string: str, \n",
    "    punctuations=r'''!()-[]{};:'\"\\,<>./?@#$%^&*_~''',\n",
    "    stop_words=['the', 'a', 'and', 'is', 'be', 'will']) -> str:\n",
    "    \"\"\"\n",
    "    A method to clean text \n",
    "    \"\"\"\n",
    "    # Cleaning the urls\n",
    "    string = re.sub(r'https?://\\S+|www\\.\\S+', '', string)\n",
    "\n",
    "    # Cleaning the html elements\n",
    "    string = re.sub(r'<.*?>', '', string)\n",
    "\n",
    "    # Removing the punctuations\n",
    "    for x in string.lower(): \n",
    "        if x in punctuations: \n",
    "            string = string.replace(x, \"\") \n",
    "\n",
    "    # Converting the text to lower\n",
    "    string = string.lower()\n",
    "\n",
    "    # Removing stop words\n",
    "    string = ' '.join([word for word in string.split() if word not in stop_words])\n",
    "\n",
    "    # Cleaning the whitespaces\n",
    "    string = re.sub(r'\\s+', ' ', string).strip()\n",
    "    string = string.split()\n",
    "    return string        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03dba212-efec-4de6-9a28-ecf4919f6903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the window for context\n",
    "window = 2\n",
    "\n",
    "# Creating a placeholder for the scanning of the word list\n",
    "word_lists = []\n",
    "all_text = []\n",
    "\n",
    "for text in sentences:\n",
    "    \n",
    "    # Cleaning the text\n",
    "    text = clean_text(text)\n",
    "    # Appending to the all text list\n",
    "    all_text += text \n",
    "\n",
    "    # Creating a context dictionary\n",
    "    for i, word in enumerate(text):\n",
    "        for w in range(window):\n",
    "            # Getting the context that is ahead by *window* words\n",
    "            if i + 1 + w < len(text): \n",
    "                word_lists.append([word] + [text[(i + 1 + w)]])\n",
    "            # Getting the context that is behind by *window* words    \n",
    "            if i - w - 1 >= 0: \n",
    "                word_lists.append([word] + [text[(i - w - 1)]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "beb1ab53-542a-4092-bd69-39a2ee601c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_unique_word_dict(text:list) -> dict:\n",
    "    \"\"\"\n",
    "    A method that creates a dictionary where the keys are unique words\n",
    "    and key values are indices\n",
    "    \"\"\"\n",
    "    # Getting all the unique words from our text and sorting them alphabetically\n",
    "    words = list(set(text))\n",
    "    words.sort()\n",
    "\n",
    "    # Creating the dictionary for the unique words\n",
    "    unique_word_dict = {}\n",
    "    for i, word in enumerate(words):\n",
    "        unique_word_dict.update({\n",
    "            word: i\n",
    "        })\n",
    "\n",
    "    return unique_word_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c07027bf-bc71-487d-84c9-83ed8389775a",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_word_dict = create_unique_word_dict(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3ca80f0-0330-4871-a99d-6d6fb0361300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'beautiful': 0,\n",
       " 'boy': 1,\n",
       " 'can': 2,\n",
       " 'children': 3,\n",
       " 'daughter': 4,\n",
       " 'family': 5,\n",
       " 'future': 6,\n",
       " 'king': 7,\n",
       " 'man': 8,\n",
       " 'now': 9,\n",
       " 'only': 10,\n",
       " 'prince': 11,\n",
       " 'princess': 12,\n",
       " 'queen': 13,\n",
       " 'realm': 14,\n",
       " 'royal': 15,\n",
       " 'rule': 16,\n",
       " 'son': 17,\n",
       " 'strong': 18,\n",
       " 'their': 19,\n",
       " 'woman': 20}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c03df7a5-7a81-44db-ab4e-c524e664f093",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d8d14b5-5087-47c8-bf04-3dec1028ef93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "84it [00:00, 82646.38it/s]\n"
     ]
    }
   ],
   "source": [
    "from scipy import sparse\n",
    "import numpy as np\n",
    "\n",
    "# Defining the number of features (unique words)\n",
    "n_words = len(unique_word_dict)\n",
    "\n",
    "# Getting all the unique words \n",
    "words = list(unique_word_dict.keys())\n",
    "\n",
    "# Creating the X and Y matrices using one hot encoding\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "for i, word_list in tqdm(enumerate(word_lists)):\n",
    "    # Getting the indices\n",
    "    main_word_index = unique_word_dict.get(word_list[0])\n",
    "    context_word_index = unique_word_dict.get(word_list[1])\n",
    "\n",
    "    # Creating the placeholders   \n",
    "    X_row = np.zeros(n_words)\n",
    "    Y_row = np.zeros(n_words)\n",
    "\n",
    "    # One hot encoding the main word\n",
    "    X_row[main_word_index] = 1\n",
    "\n",
    "    # One hot encoding the Y matrix words \n",
    "    Y_row[context_word_index] = 1\n",
    "\n",
    "    # Appending to the main matrices\n",
    "    X.append(X_row)\n",
    "    Y.append(Y_row)\n",
    "\n",
    "# Converting the matrices into an array\n",
    "X = np.asarray(X)\n",
    "Y = np.asarray(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e0d4635-b7ef-423b-9234-42b32aa0b348",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(84, 21)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f100c621-4f40-4d1b-a438-6c823e40ac43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629037a9-2c81-4c46-b5ec-b49615d86433",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/1 [==============================] - 2s 2s/step - loss: 3.0359\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 3.0349\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.0339\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.0329\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 3.0319\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 3.0309\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.0300\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 3.0290\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 3.0280\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 3.0270\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 3.0260\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 3.0251\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 3.0241\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 3.0231\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 3.0221\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 3.0212\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.0202\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 3.0192\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.0182\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 3.0173\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 3.0163\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 3.0153\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.0144\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.0134\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 3.0124\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 3.0115\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 3.0105\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 3.0095\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 3.0086\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 3.0076\n",
      "Epoch 31/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 3.0067\n",
      "Epoch 32/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 3.0057\n",
      "Epoch 33/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 3.0047\n",
      "Epoch 34/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 3.0038\n",
      "Epoch 35/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 3.0028\n",
      "Epoch 36/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.0019\n",
      "Epoch 37/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 3.0009\n",
      "Epoch 38/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 3.0000\n",
      "Epoch 39/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.9990\n",
      "Epoch 40/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 2.9981\n",
      "Epoch 41/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.9971\n",
      "Epoch 42/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 2.9961\n",
      "Epoch 43/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.9952\n",
      "Epoch 44/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.9942\n",
      "Epoch 45/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 2.9933\n",
      "Epoch 46/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.9923\n",
      "Epoch 47/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 2.9914\n",
      "Epoch 48/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 2.9904\n",
      "Epoch 49/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.9895\n",
      "Epoch 50/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.9885\n",
      "Epoch 51/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 2.9876\n",
      "Epoch 52/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 2.9866\n",
      "Epoch 53/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.9857\n",
      "Epoch 54/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 2.9847\n",
      "Epoch 55/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.9838\n",
      "Epoch 56/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.9828\n",
      "Epoch 57/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 2.9819\n",
      "Epoch 58/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 2.9809\n",
      "Epoch 59/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 2.9800\n",
      "Epoch 60/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.9791\n",
      "Epoch 61/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 2.9781\n",
      "Epoch 62/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 2.9772\n",
      "Epoch 63/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 2.9762\n",
      "Epoch 64/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.9753\n",
      "Epoch 65/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.9743\n",
      "Epoch 66/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 2.9734\n",
      "Epoch 67/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.9724\n",
      "Epoch 68/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 2.9715\n",
      "Epoch 69/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 2.9705\n",
      "Epoch 70/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 2.9696\n",
      "Epoch 71/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 2.9686\n",
      "Epoch 72/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 2.9677\n",
      "Epoch 73/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 2.9667\n",
      "Epoch 74/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 2.9658\n",
      "Epoch 75/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.9648\n",
      "Epoch 76/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 2.9639\n",
      "Epoch 77/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 2.9629\n",
      "Epoch 78/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.9620\n",
      "Epoch 79/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.9610\n",
      "Epoch 80/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 2.9601\n",
      "Epoch 81/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.9591\n",
      "Epoch 82/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.9582\n",
      "Epoch 83/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 2.9572\n",
      "Epoch 84/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 2.9563\n",
      "Epoch 85/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.9553\n",
      "Epoch 86/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.9544\n",
      "Epoch 87/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.9534\n",
      "Epoch 88/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.9525\n",
      "Epoch 89/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.9515\n",
      "Epoch 90/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 2.9506\n",
      "Epoch 91/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.9496\n",
      "Epoch 92/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.9487\n",
      "Epoch 93/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.9477\n",
      "Epoch 94/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.9468\n",
      "Epoch 95/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.9458\n",
      "Epoch 96/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.9449\n",
      "Epoch 97/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.9439\n",
      "Epoch 98/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.9430\n",
      "Epoch 99/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 2.9420\n",
      "Epoch 100/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.9411\n",
      "Epoch 101/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.9401\n",
      "Epoch 102/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.9392\n",
      "Epoch 103/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.9382\n",
      "Epoch 104/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.9372\n",
      "Epoch 105/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 2.9363\n",
      "Epoch 106/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.9353\n",
      "Epoch 107/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.9344\n",
      "Epoch 108/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.9334\n",
      "Epoch 109/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.9325\n",
      "Epoch 110/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.9315\n",
      "Epoch 111/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.9305\n",
      "Epoch 112/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.9296\n",
      "Epoch 113/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 2.9286\n",
      "Epoch 114/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 2.9277\n",
      "Epoch 115/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 2.9267\n",
      "Epoch 116/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.9257\n",
      "Epoch 117/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.9248\n",
      "Epoch 118/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 2.9238\n",
      "Epoch 119/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 2.9229\n",
      "Epoch 120/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.9219\n",
      "Epoch 121/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 2.9209\n",
      "Epoch 122/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 2.9200\n",
      "Epoch 123/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 2.9190\n",
      "Epoch 124/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.9180\n",
      "Epoch 125/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.9171\n",
      "Epoch 126/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.9161\n",
      "Epoch 127/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 2.9151\n",
      "Epoch 128/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.9142\n",
      "Epoch 129/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.9132\n",
      "Epoch 130/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 2.9123\n",
      "Epoch 131/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 2.9113\n",
      "Epoch 132/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.9103\n",
      "Epoch 133/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.9094\n",
      "Epoch 134/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 2.9084\n",
      "Epoch 135/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 2.9074\n",
      "Epoch 136/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 2.9064\n",
      "Epoch 137/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.9055\n",
      "Epoch 138/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.9045\n",
      "Epoch 139/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 2.9035\n",
      "Epoch 140/1000\n"
     ]
    }
   ],
   "source": [
    "# Deep learning: \n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Defining the size of the embedding\n",
    "embed_size = 2\n",
    "\n",
    "# Defining the neural network\n",
    "inp = Input(shape=(X.shape[1],))\n",
    "x = Dense(units=embed_size, activation='linear')(inp)\n",
    "x = Dense(units=Y.shape[1], activation='softmax')(x)\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam')\n",
    "\n",
    "# Optimizing the network weights\n",
    "model.fit(\n",
    "    x=X, \n",
    "    y=Y, \n",
    "    batch_size=256,\n",
    "    epochs=1000\n",
    "    )\n",
    "\n",
    "# Obtaining the weights from the neural network. \n",
    "# These are the so called word embeddings\n",
    "\n",
    "# The input layer \n",
    "weights = model.get_weights()[0]\n",
    "\n",
    "# Creating a dictionary to store the embeddings in. The key is a unique word and \n",
    "# the value is the numeric vector\n",
    "embedding_dict = {}\n",
    "for word in words: \n",
    "    embedding_dict.update({\n",
    "        word: weights[unique_word_dict.get(word)]\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e6e39d-9bee-4491-8940-c77549e16232",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c27ad9-b16a-4acd-a084-a6b0a9bec505",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 10))\n",
    "for word in list(unique_word_dict.keys()):\n",
    "  coord = embedding_dict.get(word)\n",
    "  plt.scatter(coord[0], coord[1])\n",
    "  plt.annotate(word, (coord[0], coord[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edce94b-4fed-450f-8ee7-39e83b0a1374",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
